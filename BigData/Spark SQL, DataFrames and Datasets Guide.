http://spark.apache.org/docs/latest/sql-programming-guide.html

DataFrame in Java: Dataset<Row>
DataFrame in Scala: Dataset[Row]


# 动手开始吧
// 初始化session
val spark = SparkSession.builder()....getOrCreate()

// 创建DataFrame
val df = spark.read.json("examples/src/main/resources/people.json")

## 操作DataFrame
http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html
```scala
df.printSchema()
df.select("name").show()
df.select($"name", $"age" + 1).show()
df.filter($"age" > 21).show()
df.groupBy("age").count().show()
```

## SQL查询
```scala
df.createOrReplaceTempView("people")
val sqlDF = spark.sql("SELECT * FROM people")
sqlDF.show()
```

## 全局临时视图(需要配置hive存储)
df.createGlobalTempView("people")
spark.sql("SELECT * FROM global_temp.people").show()
spark.newSession().sql("SELECT * FROM global_temp.people").show()

## 创建DataSet
"examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala"

## 与RDD互操作
- 方式一：通过RDD+map Person反射推断schema
val peopleDF = spark.sparkContext
  .textFile("examples/src/main/resources/people.txt")
  .map(_.split(","))
  .map(attributes => Person(attributes(0), attributes(1).trim.toInt))
  .toDF()

- 方式二：编程式
- Create an RDD of Rows from the original RDD;
- Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1.
- Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession.



## 标量函数scalar functions
http://spark.apache.org/docs/latest/sql-ref-functions.html#scalar-functions
http://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html

## 聚合函数aggregate functions

# 数据源
示例代码："examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala" 

## load/save 通用存取函数, 支持option
```scala
val df = spark.read.format("csv")
  .option("sep", ";")
  .option("inferSchema", "true")
  .option("header", "true")
  .load("examples/src/main/resources/people.csv")

usersDF.write.format("orc")
  .option("orc.bloom.filter.columns", "favorite_color")
  .option("orc.dictionary.key.threshold", "1.0")
  .option("orc.column.encoding.direct", "name")
  .save("users_with_options.orc")
```


### 直接对文件执行sql
```scala
val sqlDF = spark.sql("SELECT * FROM parquet.`examples/src/main/resources/users.parquet`")
```

### SaveMode 保存模式
SaveMode.ErrorIfExists
SaveMode.Append
SaveMode.Overwrite
SaveMode.Ignore

### saveAsTable 保存到持久化表中
df.write.saveAsTable("t")
保存到默认的warehouse directory路径中，并且将每个分区元数据存到hive元数据存储中。

df.write.option("path", "/some/path").saveAsTable("t")
保存到指定的路径中。
但这种方式不会将metadata存到hive元数据存储中，需要你手工同步到hive：MSCK REPAIR TABLE.

读取出来
spark.table("people").show()

分桶、排序、分区:
分桶、排序只能应用于持久化表: bucketedBy, sortBy ==> saveAsTable
分区 partitionBy ==> save和saveAsTable

### 通用文件选项
基于文件的数据源：parquet, orc, avro, json, csv, text

通用选项：spark.sql.files
- 忽略受损的文件 ignoreCorruptFiles
- 忽略丢失的文件 ignoreMissingFiles
- 路径全局过滤 pathGlobFilter
- 文件递归查找 recursiveFileLookup （与 partitionSpec选项互斥）


### Parquet文件格式
Partition Discovery:
spark.sql.sources.partitionColumnTypeInference.enabled=true(default)
path/to/table/gender=male

Schema Merging: mergeSchema

### Hive metastore parquet table 的转换
写非分区的Hive metastore Parquet table时，有2种方式：
默认用自己的Parquet support，性能好一些;
还有就是Hive SerDe;
spark.sql.hive.convertMetastoreParquet=true

#### Hive/Parquet schema 协调
Hive是大小写敏感，Parquet不是
Hive认为所有列都是可空的，而while nullability in Parquet is significant

#### 元数据刷新
为了提升性能，SparkSQL 会缓存Parquet的元数据，为了保证元数据的一致性，你可以手工刷新：
spark.catalog.refreshTable("my_table")


编程式，设置选项：spark.setConf("key", value)
SQL，设置选项: SET key=value 

### ORC Files
spark.sql.orc.impl=native
spark.sql.orc.enableVectorizedReader=true

### Hive Tables





