

## 02 | MapReduce后谁主沉浮：怎样设计下一代数据处理技术？

- 我们需要一种技术抽象让多步骤数据处理变得易于维护：DAG

- 我们不想要复杂的配置，需要能自动进行性能优化

- 我们要能把数据处理的描述语言，与背后的运行引擎解耦合开来

- 我们要统一批处理和流处理的编程模型

批处理一套api和pipline
流处理一套api和pipline
需要维护两套，一旦业务需求变化，就得大改甚至重做 
===> 解法：流批一体化，即使业务需求变化，开发者也不用频繁修改代码

- 我们要在架构层面提供异常处理和数据监控的能力

大规模数据处理工程实践难点：异常监控、诊断、处理


## 03 | 大规模数据处理初体验：怎样实现大型电商热销榜

例子 --> 单机方案 --> 应对大数据量: 集群多步骤流水线 --> 应对场景变化：框架

## 04 | 分布式系统（上）：学会用服务等级协议SLA来评估你的系统

1. 可用性（Availabilty）

2. 准确性（Accuracy）：错误率（Error Rate）= 导致系统内部错误的有效请求数/期间的有效请求总数
准确性指的是我们所设计的系统服务中，是否允许某些数据是不准确的或者是丢失了的。
如果允许这样的情况发生，用户可以接受的概率（百分比）是多少？

Google Cloud Platform 的 SLA 中，有着这样的准确性定义：每个月系统的错误率超过 5% 的时间要少于 0.1%，以每分钟为单位来计算。

而亚马逊 AWS 云计算平台有着稍微不一样的准确性定义：以每 5 分钟为单位，错误率不会超过 0.1%。你看，我们可以用错误率来定义准确性，但具体该如何评估系统的准确性呢？一般来说，我们可以采用性能测试（Performance Test）或者是查看系统日志（Log）两种方法来评估。


3. 系统容量(Capacity)：QPS
想要得到系统能承受的最大 QPS，更多的是性能测试和日志分析相结合的手段。

4. 延迟(Latency): P99 1秒

在定义延迟的 SLA 时，我们常常看到系统的 SLA 会有 p95 或者是 p99 这样的延迟声明。这里的 p 指的是 percentile，也就是百分位的意思。如果说一个系统的 p95 延迟是 1 秒的话，那就表示在 100 个请求里面有 95 个请求的响应时间会少于 1 秒，而剩下的 5 个请求响应时间会大于 1 秒。

## 05 | 分布式系统（下）：架构师不得不知的三大指标
CAP 理论：一致性、可用性、分区

可扩展性(Scalability)
一致性(Data Consistency)
持久性(Data Durability)


## 06 | 如何区分批处理还是流处理？
有界，无界

## 07 | Workflow设计模式：让你在大规模数据世界中君临天下
工作流系统的设计模式：
- Copier Pattern 复制模式（视频分辨率、语言）
- Filter Pattern 过滤模式
- Splitter Pattern 分离模式
- Joiner Pattern 合并模式。

## 08 | 发布/订阅模式：流处理架构中的瑞士军刀

## 10 | Lambda架构：Twitter亿级实时数据分析架构背后的倚天剑
Lambda 架构总共由三层系统组成：
批处理层（Batch Layer），
速度处理层（Speed Layer），
以及用于响应查询的服务层（Serving Layer）。

例子：找车位软件

## 11 | Kappa架构：利用Kafka锻造的屠龙刀
Lambda架构的问题：需要维护两套系统，要保证两套系统计算逻辑一致。

Jay Kreps问了2个问题，于是有了Kappa架构:
我们能不能改进 Lambda 架构中速度层的系统性能，使得它也可以处理好数据的完整性和准确性问题呢？
我们能不能改进 Lambda 架构中的速度层，使它既能够进行实时数据处理，同时也有能力在业务逻辑更新的情况下重新处理以前处理过的历史数据呢？


Kappa架构缺陷：
因为 Kappa 架构只保留了速度层而缺少批处理层，在速度层上处理大规模数据可能会有数据更新出错的情况发生，
这就需要我们花费更多的时间在处理这些错误异常上面。

小结：
如果你所面对的业务逻辑是设计一种稳健的机器学习模型来预测即将发生的事情，那么你应该优先考虑使用 Lambda 架构，因为它拥有批处理层和速度层来确保更少的错误。

如果你所面对的业务逻辑是希望实时性比较高，而且客户端又是根据运行时发生的实时事件来做出回应的，那么你就应该优先考虑使用 Kappa 架构。

## 12 | 我们为什么需要Spark？

Hadoop的问题：
1. MapReduce抽象层次低，大量的底层逻辑都需要开发者手工完成。
2. 只提供Map和Reduce，操作对开发者非常不友好，维护一个多任务协调的状态机成本很高
3. 在Hadoop中，每个Job的计算结果都存储到HDFS文件系统中，大量的IO导致系统延迟很高。
由于这个原因，MapReduce对迭代算法的处理性能都很差，而且很消耗资源：每次迭代都要进行HDFS读写。
4. 只支持批处理数据，欠缺对数据流处理的支持。

Spark特性
1. RDD编程模型提供了简单的api.
2. 快：不用每次都读写HDFS，大部分时候都是内存计算.
3. 从狭义上来看，Spark 只是 MapReduce 的替代方案，大部分应用场景中，它还要依赖于 HDFS 和 HBase 来存储数据，依赖于 YARN/K8s/standalone 来管理集群和资源。

任务模型对比：
在任务（task）级别上，Spark 的并行机制是多线程模型，而 MapReduce 是多进程模型。
MapReduce多进程模型便于细粒度控制每个任务占用的资源，但会消耗较多的启动时间。
Spark多线程模型，使得多个任务运行在同一个JVM进程中，可以带来更快的启动速度、更高的SPU利用率，以及更好的内存共享。


## 13 | 弹性分布式数据集：Spark大厦的地基（上）
RDD 弹性分布式数据集：：分区、不可变、并行操作
分布式数据的抽象模型 + 操作api

SparkContext 功能入口，代表与spark的连接，具有生命周期。
SparkConf 配置
Partitions 分区，每个partition会映射到节点内存或硬盘的一个数据块。
Partitioner 决定了分区方式，目前主流两种分区方式：Hash Partitioner, Range Partitioner
Dependencies 依赖关系：Narrow Dependency / Wide Dependency
显然，窄依赖允许子 RDD 的每个分区可以被并行处理产生，
而宽依赖则必须等父 RDD 的所有分区都被计算好之后才能开始处理。
Checkpoint 检查点：减少故障恢复的代价
Prefered Location
Storage Level 
Iterator

为什么要区分宽/窄依赖关系？
1. 窄依赖支持在同一个数据节点上链式执行多个命令，例如filter-> map；
宽依赖要求所有父分区都是可用的，而且可能会需要reduce之类的操作进行跨节点数据传递(shuffle)；

2. 从失败恢复角度考虑，窄依赖的失败恢复更加简单，只需要重新计算丢失的父分区即可；
宽依赖牵涉到RDD各级的多个父分区；

RDD的特性：
- 尽可能存在内存中，极大提高计算效率
- 做了分区，天然支持并行处理
- 建立了父子依赖关系，大大提升了容错性和失败恢的效率

## 14 | 弹性分布式数据集：Spark大厦的地基（下）

RDD操作api分2种： 

1. Transformation(RDD -> RDD) 生成RDD和依赖关系DAG：
map, mapPartitions, filter, groupByKey

2. Action(RDD -> result)：触发计算
collect, count, countByKey

关键词：Lazy执行 + 反向递归

性能提升：缓存（持久化）: cache(), persist()

缓存（持久化）与 Checkpoint机制的区别是什么？
主要区别应该是对依赖链的处理：
checkpoint在action之后执行，相当于事务完成后备份结果。既然结果有了，之前的计算过程，也就是RDD的依赖链，也就不需要了，所以不必保存。
但是cache和persist只是保存当前RDD，并不要求是在action之后调用。相当于事务的计算过程，还没有结果。既然没有结果，当需要恢复、重新计算时就要重放计算过程，自然之前的依赖链不能放弃，也需要保存下来。需要恢复时就要从最初的或最近的checkpoint开始重新计算。


## 15 | Spark SQL：Spark数据查询的利器

Hive：简化MapReduce的任务编排，提供类SQL的编程接口，HQL经过语法解析、逻辑计算、物理计划转换成MapReduce程序执行。

SparkSQL: 将SQL转化成RDD的操作，并且提供两套API：DataFrame API 和 DataSet API。

### 对比 RDD, DataSet, DataFrame
和RDD的父子依赖类似，DataSet 的内部结构包含了逻辑计划，即生成该数据集所需要的运算。
当实际执行计算时，SparkSQL的查询优化器会优化这个逻辑，并生成一个可分布式执行的，包含分区信息的物理计划。

发展历史: RDD -> DataFrame(for SQL) -> DataSet

RDD：数据处理的基石，感知的对象类型，操作也是整个对象。
DataFrame：基于RDD，没有类型信息，所以无法做编译期的类型检查。
DataSet：基于RDD，做了优化：感知schema，包括详细的结构信息与每列的数据类型，所以，在程序编译时可以执行类型检测，而且能对操作进行优化。

[RDD-Dataset-DataFrame对比](RDD-Dataset-DataFrame对比.jpg)

### 总结
DataFrame和DataSet基于RDD做了优化。

但是RDD API 对于非结构化的数据处理有独特的优势，比如文本流数据，而且更方便我们做底层的操作。所以在开发中，我们还是需要根据实际情况来选择使用哪种 API。

## 16 | Spark Streaming：Spark的实时流计算API

Spark Strreaming提供了一个对流式数据的抽象 DStream.
DStream 的来源可以是 Kafka, Flume, HDFS数据流，或者说其他DStream

DStream底层也是基于RDD构成，对DStream的操作会被转换成对RDD的操作。
DStream内部是一个连续的RDD序列，每个RDD代表一个时间窗口的输入数据流。

DStream除了支持所有RDD的操作外，还有一些特殊操作：
- 滑动窗口操作: countByWindow, xxByWindow
例子：10秒统计一次60秒内的热词
基本参数：window length,  sliding length

### 总结
基本思想：基于RDD实现，用micro-batch模拟流。

**优点**
RDD的优点都继承了，与Spark核心引擎、SparkSQL、MLib等无缝衔接，容易扩展
，处理速度快、良好容错恢复能力、支持高度并行计算；

**缺点**
不能支持太小的批处理时间间隔，延迟高于Storm, Flink。
Structured streaming对此作了一定改进，是Spark流处理的未来。

## 17 | Structured Streaming：如何用DataFrame API进行实时数据分析?
Structured Streaming 对无边界的数据进行建模和处理

在Spark Streaming模型中，数据流按一定间隔被分割成小的数据块进行批处理。

在Structured Streaming模型中，数据流被看做是一个无边界的关系型数据表，每条数据不断被追加到表中。

与Spark Streaming类似，Structured Streaming也是将数据流 --> 按时间间隔切分数据段 --> 数据段写入表

我们针对该数据流的查询（计算）相当于是订阅了数据表，随着新数据的到达，查询结果会被不断触发更新。
输出结果也是表的形式，可以写入外部存储（硬盘或者HDFS等）。

**outputMode输出结果的模式**
1. Complete Mode 完全模式，整个更新过的输出表都被写入外部存储；
2. Append Mode 追加模式，上次触发之后新增的行 才写入外部存储，如果老数据有改动则不合适这个模式；
3. Update Mode 更新模式，上次触发之后更新的行 才写入外部存储。

需要注意的是， Structured Streaming 并不会完全存储输入数据。每个时间间隔他都会读取最新的输入，进行处理，更新输出表，然后把这次的输入删除。
Structured Streaming 只会存储更新输出表所需要的信息。

Spark Streaming只能基于处理时间（Processing Time）处理数据。
Structured Streaming 的模型在按事件时间（Event Time）处理数据时非常方便。

### DataFrame API增加了对流式数据支持
在 Structured Streaming 发布以后，DataFrame 既可以代表静态的有边界数据，也可以代表无边界数据。
之前对静态 DataFrame 的各种操作同样也适用于流式 DataFrame。

- 创建DataFrame
sparkSession.readStream()返回的DataStreamReader可以用于创建流数据DataFrame，支持多种类型的输入：文件，kafka，socket等。
```Scala
socketDataFrame = sparkSession
    .readSream
    .format("socket")
    .option("host", "localhost")
    .option("port", 8888)
    .load()
```
- 基本查询: select ,where ,groupBy, sortXXX, head ....

- 滑动窗口window: wordsDataFrame.groupBy( window(...) ).count()

- 输出结果流
用Dataset.writeStream()返回的DataStreamWriter输出结果，支持多种写入目标：文件，kafka，console，内存等。
```Scala
query = wordCounts
   .writeStream
   .outputMode("complete")
   .format("csv")
   .option("path", "path/to/destination/dir")
   .start()

query.awaitTermination()
```

###  Structured Streaming与Spark Streaming对比

使用难度和性能
实时性
对事件时间的支持

### 实际例子
18 | Word Count：从零开始运行你的第一个Spark应用
19 | 综合案例实战：处理加州房屋信息，构建线性回归模型
20 | 流处理案例实战：分析纽约市出租车载客信息

数据清洗：
schema定义：StructType + StructField
强制类型转换: withColumn(fieldName, field).cast(field.dataType)

Stream-Stream Join 
难点：任意时刻，数据流都是不完整的，但是把旧数据一直保存在内存中 会内存overflow。
解法：引入水位 Watermark，定义了我们可以对数据延迟的最大容忍限度。
所以Stream-Stream Join是有容量限制的。

## 21 | 深入对比Spark与Flink：帮你系统设计两开花
Spark还不够实时：Spark 的流处理是基于所谓微批处理（Micro-batch processing）的思想，即它把流处理看作是批处理的一种特殊形式，每次接收到一个时间间隔的数据才会去处理，所以天生很难在实时性上有所提升。

虽然在 Spark 2.3 中提出了连续处理模型（Continuous Processing Model），但是现在只支持很有限的功能，并不能在大的项目中使用。

### Flink 核心模型简介
[Flink架构图](大规模数据处理实战-Flink架构.png)

Flink：stream native, Stream是核心数据模型，静态数据看做是Stream的特例。
Spark：micro-batch as Stream, RDD静态批数据是核心数据模型，流式数据看做是一种特例：spark内部把流转micro-batch处理。

Flink：Streaming Dataflow = Stream + Operator + Source + Sink

Flink Stream 数据传输两种形式：
- One-to-One 一对一
- Redistributing 重新分布

Flink提供两套核心API：DataStream API + DataSet API


### Flink 和 Spark 对比

通过前面的学习，我们了解到，Spark 和 Flink 都支持批处理和流处理，接下来让我们对这两种流行的数据处理框架在各方面进行对比。

**首先，这两个数据处理框架有很多相同点：**
- 都基于内存计算；
- 都有统一的批处理和流处理 API，都支持类似 SQL 的编程接口；
- 都支持很多相同的转换操作，编程都是用类似于 Scala Collection API 的函数式编程模式；
- 都有完善的错误恢复机制；
- 都支持 Exactly once 的语义一致性。


**当然，它们的不同点也是相当明显，我们可以从 4 个不同的角度来看：**
- 从流处理的角度来讲，
Spark 基于微批量处理，把流数据看成是一个个小的批处理数据块分别处理，所以延迟性只能做到秒级。而 Flink 基于每个事件处理，每当有新的数据输入都会立刻处理，是真正的流式计算，支持毫秒级计算。由于相同的原因，Spark 只支持基于时间的窗口操作（处理时间或者事件时间），而 Flink 支持的窗口操作则非常灵活，不仅支持时间窗口，还支持基于数据本身的窗口，开发者可以自由定义想要的窗口操作。

- 从 SQL 功能的角度来讲，
Spark 和 Flink 分别提供 SparkSQL 和 Table API 提供 SQL 交互支持。两者相比较，Spark 对 SQL 支持更好，相应的优化、扩展和性能更好，而 Flink 在 SQL 支持方面还有很大提升空间。

- 从迭代计算的角度来讲，Spark 对机器学习的支持很好，因为可以在内存中缓存中间计算结果来加速机器学习算法的运行。但是大部分机器学习算法其实是一个有环的数据流，在 Spark 中，却是用无环图来表示。而 Flink 支持在运行时间中的有环数据流，从而可以更有效的对机器学习算法进行运算。

- 从相应的生态系统角度来讲，
Spark 的社区无疑更加活跃。Spark 可以说有着 Apache 旗下最多的开源贡献者，而且有很多不同的库来用在不同场景。而 Flink 由于较新，现阶段的开源社区不如 Spark 活跃，各种库的功能也不如 Spark 全面。但是 Flink 还在不断发展，各种功能也在逐渐完善。

## 22 | Apache Beam的前世今生
大规模数据处理需求 --> 重复造轮子 --> MapReduce --> FlumeJava用户友好的分布式处理 --> Millwheel支持流 --> Datflow综合平衡各种模型优点 --> Apache Beam多平台

Beam 提供了一套统一的 API 来处理这两种数据处理模式，让我们只需要将注意力专注于在数据处理的算法上，而不用再花时间去对两种数据处理模式上的差异进行维护。

## 23 | 站在Google的肩膀上学习Beam编程模型
我们已经知道，这个世界中的数据可以分成有边界数据和无边界数据，而有边界数据又是无边界数据的一种特例。所以，我们都可以将所有的数据抽象看作是无边界数据。同时，每一个数据都是有两种时域的，分别是事件时间和处理时间。我们在处理无边界数据的时候，因为在现实世界中，数据会有延时、丢失等等的状况发生，我们无法保证现在到底是否接收完了所有发生在某一时刻之前的数据。**所以现实中，流处理必须在数据的完整性和数据处理的延时性上作出取舍。**

Beam 编程模型就是在这样的基础上提出的：窗口window，水位watermark，触发器trigger，累计accumulation
WWWH：

## 24 | PCollection：为什么Beam要如此抽象封装数据？
PCollection + Coders + Transformation

**PCollections: 并行数据集（和spark RDD类似)**
无序：分布式
没有固定大小：统一处理流批，但按数据源分为有界和无界
不可变immutable


**ParDo + DoFn**
pcollection.apply(ParDo.of(new DoFn()))

**Statefulness 与 side input/side output**

ParDo 与spark的map，MapReduce中的map非常相似，但出统一计算模型的考虑，也有很大的不同：
1. ParDo支持数据输出到多个PCollection，而Spark和MapReduce的map可以说是单线的。
2. ParDo提供内建的状态存储机制，而Spark和MapReduce没有（Spark Streaming有mapWithState ）。


## 26 | Pipeline：Beam如何抽象多步骤的数据流水线？
数据流水线 Pipepline
```Java
PipelineOptions options = PipelineOptionsFactory.create();
Pipeline p = Pipeline.create(options);
```

Bundles+Wokers

## 27 | Pipeline I/O: Beam数据中转的设计模式

### Read Transform 读取数据集
```Java
PCollection<String> inputs = pipeline.apply(TextIO.read().from(filePath));
```

还支持从不同数据源读取同一类型的数据作为输入数据集，
首先多次调用Read Stream 读取来源不同的数据，然后利用 flatten 将数据集合并：
```Java
PCollection<String> input1 = pipeline.apply(TextIO.read().from(filePath1));
PCollection<String> input2 = pipeline.apply(TextIO.read().from(filePath2));
PCollection<String> input3 = pipeline.apply(TextIO.read().from(filePath3));

PCollectionList<String> collections = PCollectionList.of(input1).and(input2).and(input3);
PCollection<String> inputs = collections.apply(Flatten.<String>pCollections());
```

### Write Transform 输出数据集 

输出到指定目标
```Java
outputCollection.apply(TextIO.write().to(filePath/output));
```

withSuffix 保存为特定类型的文件格式
```Java
outputCollection.apply(TextIO.write().to(filePath/output).withSuffix(".csv"));
```

### Beam 怎么扩展支持不同格式的输入输出呢？

Read和Write的 Transform 操作都是在名为 I/O Connector 的类中实现的.
Beam原生支持的 I/O Connector 涵盖了大部分场景：FileIO, TFRecordIO, KafakaIO, PubSubIO, JdbcIO, RedisIO 等

自定义 I/O Connector:
    自定义对有界数据集的读取：
    1）ParDo 和 GroupByKey 来模拟读取数据的逻辑； 
    2）继承BoundedSource

    自定义对无界数据集的读取：UnboundedSource


    自定义输出操作：只需要在ParDo 中调用对应文件系统（一切接文件：console、文件、数据库、网络等）写操作API

    

## 28 | 如何设计创建好一个Beam Pipeline？
## 29 | 如何测试Beam Pipeline？
一般来说，Transform 的单元测试可以通过以下五步来完成：
1. 创建一个 Beam 测试 SDK 中所提供的 TestPipeline 实例。
2. 创建一个静态（Static）的、用于测试的输入数据集。
3. 使用 Create Transform 来创建一个 PCollection 作为输入数据集。
4. 在测试数据集上调用我们需要测试的 Transform 上并将结果保存在一个 PCollection 上。
5. 使用 PAssert 类的相关函数来验证输出的 PCollection 是否是我所期望的结果。

## 30 | Apache Beam实战冲刺：Beam如何run everywhere?
Apache Beam 的Pipeline 串起来看一下：
```Java
//创建pipeline
PiplineOptions options = PipelineOptionsFactory.create();
Pipeline pipeline = Pipeline.create(options);

//读取数据
PCollection<String> lines = pipeline.apply("ReadLines", TextIO.read().from("gs://some/inputData.txt"));

//数据处理：PCollection --> PCollection
PCollection<String> filteredLines = lines.apply(new FiltereLines());

//输出数据
filteredLines.apply("WriteMyFile", TextIO.write().to("gs://some/outputData.txt"));

//触发执行
pipeline.run().waitUntilFinish()

```

想要在跑在不同的执行引擎上，只需要跟换options即可：FlinkPipelineOptions/SparkPipelineOptions/...
```Java
options = PipelineOptionsFactory.as(SparkPipelineOptions.class);
Pipeline pipeline = Pipeline.create(options);
```

更常见的方式是从命令行中的参数来创建PipelineOption:
```Java

public static void main(String[] args) {
     PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();
     Pipeline p = Pipeline.create(options);
}
```


## 31 | WordCount Beam Pipeline实战
## 32 | Beam Window 打通流处理的任督二脉
## 33 | Beam Window 窗口在流处理的场景中的应用
## 34 | Amazon热销版
## 35 | Facebook游戏试试流处理（上）
## 34 | Facebook游戏试试流处理（下）
为什么要有窗口呢？我们想要根据时间戳来分组处理流式数据。

有这么三种窗口
- FixedWindow：固定窗口
- SlidingWindow：滑动窗口
- SessionWindow：俩人聊天，有一搭没一搭聊，如果隔太久断掉了，重新聊天就算一个新会话。

```Java
PCollection<String> input = p.apply(KafkaIO.<Long, String>read()).apply(Values.<String>create());
PCollection<String> fixedWindowedInputs = input.apply(Window.<String>into(FixedWindows.of(Duration.standardHours(1))));
```

### 6个问题
怎样区分有界数据还是无界数据？
怎样读取无边界数据？
怎样给 PCollection 数据添加时间戳？
怎样在 PCollection 应用窗口？
怎样复用之前的 DoFn 和 PTransform？
怎样存储无边界数据？

在 Beam 中，我们可以为 PCollection 设置的触发器有 4 种模式：
1. 基于事件时间的触发器（Event-Time Trigger），在Beam中可以使用AfterWatermark显示设置。
2. 基于处理时间的触发器（Process-Time Trigger)，在Beam中可以使用AfterProcessingTime显示设置。
3. 数据驱动的触发器（Data-Driven Trigger)，在Beam中可以使用 AfterPane.elementCountAtLeast() 函数来配置。
4. 复合触发器（Composite Trigger），基于上面的三种基本触发器组合而成。

- 如果现实中只有一个数据到达窗口，那岂不是永远都触发不了计算了？
其实，这个时候就可以定义一个复合触发器，可以定义成累积有超过两个元素落入窗口中或者是每隔一分钟触发一次计算的复合触发器。

- 于“迟到”的数据，在Beam中又是如何处理呢？
我们可以使用withAllowedLateness这个在Window类中定义好的函数：
```Java
public Window<T> withAllowedLateness(Duration allowedLateness);
```

- Beam中可以设置两种累加模式：
    - 丢弃模式: Window类中函数 discardingFiredPanes()
    - 累积模式: Window类中函数 accumulatingFiredPanes()




几篇论文地址：
0. MapReduce: https://research.google.com/archive/mapreduce-osdi04.pdf
1. Flumejava: https://research.google.com/pubs/archive/35650.pdf
2. MillWheel: https://research.google.com/pubs/archive/41378.pdf
3. Data flow Model: https://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf